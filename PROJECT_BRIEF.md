# DataRoom Intelligence Bot v2.0 - Professional RAG Architecture
## Project Brief for Greenfield Implementation

---

## Executive Summary

Complete architectural redesign of the DataRoom Intelligence Bot to achieve commercial-grade quality for venture capital firms. The current system fails due to lack of proper data preprocessing pipeline, resulting in poor extraction quality and unreliable Q&A functionality. This document outlines a complete rebuild using professional RAG (Retrieval-Augmented Generation) architecture.

**Core Principle**: The problem is not the AI model (GPT-4o), but the data engineering pipeline that feeds it.

---

## 1. Problem Diagnosis

### 1.1 Current State Analysis

The existing application attempts direct PDF-to-GPT processing without proper preprocessing, resulting in:

- **Loss of Document Structure**: PDFs are complex visual formats with tables, columns, headers, and hierarchical content. Direct API calls flatten this into unstructured text.
- **Poor Extraction Quality**: <70% of financial metrics captured correctly
- **Unreliable Q&A**: Responses lack context and miss critical information
- **No Persistence**: Each session starts fresh, no accumulation of knowledge
- **Single-tenant Design**: Cannot handle multiple datarooms simultaneously

### 1.2 Root Cause

Research reveals the "UI Magic" phenomenon: ChatGPT and similar UIs achieve superior results through sophisticated preprocessing pipelines that:
1. Parse document structure (Document Layout Analysis)
2. Preserve formatting (tables, lists, hierarchies)
3. Convert to structured formats (Markdown)
4. Chunk intelligently (structure-aware segmentation)
5. Enable semantic search (vector embeddings)

**Current App**: `PDF ‚Üí GPT-4o ‚Üí Poor Results`
**Required Pipeline**: `PDF ‚Üí Parse ‚Üí Structure ‚Üí Chunk ‚Üí Vectorize ‚Üí RAG ‚Üí GPT-4o ‚Üí Quality Results`

### 1.3 Quality Gap Evidence

Testing outputs (220925_01, 220925_02) demonstrate:
- Incomplete financial data extraction
- Missing slide references
- Inability to cross-reference information
- Inconsistent formatting
- Lost tabular data

---

## 2. Proposed Architecture: Single-Tenant RAG Pipeline

### 2.1 High-Level Architecture

**Single-Tenant Design**: Each VC firm gets their own dedicated instance with complete data isolation.

**Within each instance**: Multiple Slack channels represent different deals/datarooms.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          SINGLE-TENANT RAG SYSTEM (Per VC Firm)              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  Slack Commands              Channel-Based Datarooms         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  /load       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Dataroom Manager     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  /use        ‚îÇ             ‚îÇ                         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  /stop       ‚îÇ             ‚îÇ  Channel ‚Üí VectorDB     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  /current    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ/list-datarooms‚îÇ             ‚îÇ  ‚îÇ #startup-a        ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ                                ‚îÇ  ‚îÇ ‚îÇ   VectorDB    ‚îÇ ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ  ‚îÇ ‚îÇ   (startup-a) ‚îÇ ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  /summary    ‚îÇ             ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  /ask        ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ    ‚îÇ
‚îÇ        ‚ñ≤                       ‚îÇ  ‚îÇ #startup-b        ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ        ‚îÇ                       ‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ        ‚îÇ                       ‚îÇ  ‚îÇ ‚îÇ   VectorDB    ‚îÇ ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ        ‚îÇ                       ‚îÇ  ‚îÇ ‚îÇ   (startup-b) ‚îÇ ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ        ‚îÇ                       ‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ        ‚îÇ                       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ    ‚îÇ
‚îÇ        ‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ        ‚îÇ                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                             ‚îÇ
‚îÇ  ‚îÇ Active       ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ Channel      ‚îÇ              ‚îÇ     RAG Pipeline        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ                         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ #startup-a   ‚îÇ              ‚îÇ 1. Parse (LlamaParse)   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ              ‚îÇ 2. Chunk (Structure)    ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ 3. Embed (OpenAI)      ‚îÇ    ‚îÇ
‚îÇ                                 ‚îÇ 4. Store (ChromaDB)    ‚îÇ    ‚îÇ
‚îÇ                                 ‚îÇ 5. Retrieve (Semantic) ‚îÇ    ‚îÇ
‚îÇ                                 ‚îÇ 6. Generate (GPT-4o)   ‚îÇ    ‚îÇ
‚îÇ                                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ                DEPLOYMENT MODEL                         ‚îÇ
   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
   ‚îÇ                                                         ‚îÇ
   ‚îÇ  Sequoia Capital          Andreessen Horowitz          ‚îÇ
   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
   ‚îÇ  ‚îÇ RAG Instance    ‚îÇ      ‚îÇ RAG Instance    ‚îÇ          ‚îÇ
   ‚îÇ  ‚îÇ #startup-a      ‚îÇ      ‚îÇ #deal-1         ‚îÇ          ‚îÇ
   ‚îÇ  ‚îÇ #startup-b      ‚îÇ      ‚îÇ #deal-2         ‚îÇ          ‚îÇ
   ‚îÇ  ‚îÇ #startup-c      ‚îÇ      ‚îÇ #deal-3         ‚îÇ          ‚îÇ
   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
   ‚îÇ                                                         ‚îÇ
   ‚îÇ          Accel Partners                                 ‚îÇ
   ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
   ‚îÇ          ‚îÇ RAG Instance    ‚îÇ                           ‚îÇ
   ‚îÇ          ‚îÇ #portfolio-1    ‚îÇ                           ‚îÇ
   ‚îÇ          ‚îÇ #portfolio-2    ‚îÇ                           ‚îÇ
   ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Technology Stack (Single-Tenant)

#### Core Components
- **PDF Parsing**: LlamaParse API (best quality/speed/cost balance)
- **Vector Database**: ChromaDB with channel-based collections (simple, fast)
- **Embeddings**: OpenAI text-embedding-3-small
- **LLM**: GPT-4o for synthesis
- **Framework**: LlamaIndex for RAG orchestration
- **Backend**: Flask + Slack Bolt (maintain existing)
- **Deployment**: Single instance per VC firm (complete isolation)

#### Key Libraries
```python
llama-parse==0.3.0          # PDF to Markdown conversion
llama-index==0.9.0          # RAG framework
chromadb==0.4.0             # Vector database
openai==1.0.0               # Embeddings and GPT-4o
pydantic==2.0.0             # Data validation
```

### 2.3 Simplified Data Flow Architecture

```python
# Single-tenant data flow
class ChannelDataroomPipeline:
    """
    Channel + PDF Documents
         ‚Üì
    LlamaParse (PDF ‚Üí Markdown)
         ‚Üì
    Structure Preservation
         ‚Üì
    Smart Chunking (MarkdownElementNodeParser)
         ‚Üì
    Embedding Generation (text-embedding-3-small)
         ‚Üì
    Vector Storage (ChromaDB collection = f"channel_{channel_id}")
         ‚Üì
    RAG Retrieval (Semantic Search + Re-ranking)
         ‚Üì
    GPT-4o Synthesis (Context-aware generation)
         ‚Üì
    Structured Response
    """
```

---

## 3. Command Architecture

### 3.1 Command Specifications

#### `/load [google-drive-url] [optional: dataroom-name]`
**Purpose**: Load documents into current channel's dataroom (simple channel = dataroom mapping)
**Behavior**: 
- Auto-generates dataroom name if not provided
- Uses custom name if provided
- REPLACES entire content if dataroom exists in this channel
- Processes all documents in drive folder
- Creates structured vector database for this channel only
- **Immediately ready for analysis** in this channel

**Response Format**:
```
# Option A: Auto-generated name
/load https://drive.google.com/startup-x
üì• Loading dataroom...
‚úÖ Successfully processed:
‚Ä¢ pitch_deck_v2.pdf (45 pages, 127 chunks)
‚Ä¢ financial_model.xlsx (12 sheets, 43 chunks)  
‚Ä¢ cap_table.pdf (8 pages, 22 chunks)

üìä Dataroom 'startup_x' loaded and ready (3 documents, 192 chunks)
üîó Use /summary and /ask commands

# Option B: Custom name
/load https://drive.google.com/startup-x "Startup X Series A"
üìä Dataroom 'Startup X Series A' loaded and ready (3 documents, 192 chunks)
üîó Use /summary and /ask commands
```

#### `/current`
**Purpose**: Show current channel's dataroom info
**Response Format**:
```
üìä Current dataroom: 'Startup X Series A'
‚Ä¢ Loaded: Jan 15 at 14:30
‚Ä¢ Documents: 3 files
‚Ä¢ Ready for analysis
```

#### `/list-datarooms`
**Purpose**: List all datarooms across all channels in this workspace
**Response Format**:
```
üìÅ Datarooms in this workspace:
‚Ä¢ Startup X Series A in #startup-x-dd
  Created: Jan 15

‚Ä¢ Startup Y Seed in #startup-y-dd
  Created: Jan 10

‚Ä¢ Portfolio Company Z in #portfolio-review
  Created: Jan 12

Switch to any channel and use /summary or /ask
```

#### `/summary`
**Purpose**: Generate comprehensive VC analysis of connected dataroom
**Behavior**:
- **Requires active dataroom connection** - fails if no dataroom connected to channel
- Queries entire dataroom context
- Generates structured investment summary

**Response Format**:
```
üìä DATAROOM ANALYSIS - Startup X Series A

EXECUTIVE SUMMARY
[Company description, traction, funding round]

COMPANY & PRODUCT
‚Ä¢ [What they do] [Source: pitch_deck.pdf, p.3]
‚Ä¢ [Key differentiation] [Source: pitch_deck.pdf, p.7]

FINANCIAL METRICS
‚Ä¢ Revenue: ‚Ç¨2M (2023), ‚Ç¨5M (2024 proj) [Source: pitch_deck.pdf, p.12]
‚Ä¢ Burn Rate: ‚Ç¨150k/month [Source: financial_model.xlsx]
‚Ä¢ Runway: 18 months [Source: pitch_deck.pdf, p.20]

[... continues with structured sections ...]
```

#### `/ask [question]`
**Purpose**: Interactive Q&A on connected dataroom
**Behavior**:
- **Requires active dataroom connection** - fails if no dataroom connected to channel
- Semantic search across all documents in connected dataroom
- Returns answer with source attribution

**Example**:
```
User: /ask What's the LTV/CAC ratio?

Bot: Based on the connected dataroom (Startup X Series A):
The LTV/CAC ratio is 3.2x [Source: pitch_deck.pdf, page 15]
‚Ä¢ LTV: ‚Ç¨960 per customer
‚Ä¢ CAC: ‚Ç¨300 blended
```

### 3.2 Single-Tenant Dataroom Architecture

```python
class DataroomManager:
    """
    Single-Tenant Design: One instance per VC firm
    Channel-Based: Each channel = one dataroom
    Simple & Secure: Complete isolation per client
    """
    def __init__(self):
        # Simple channel-based storage (no multi-tenancy complexity)
        self.datarooms = {}  # channel_id -> Dataroom instance
        
        # Metadata for management
        self.channel_metadata = {}  # channel_id -> {"name": "Startup X", "created_at": "..."}
        
    def get_dataroom(self, channel_id: str) -> Optional[Dataroom]:
        """Get dataroom for a channel (creates if doesn't exist)"""
        return self.datarooms.get(channel_id)
        
    def load_dataroom(self, channel_id: str, drive_url: str, custom_name: str = None):
        """Load documents into channel's dataroom (simple 1:1 mapping)"""
        # Generate display name
        display_name = custom_name or self.generate_dataroom_name(drive_url)
        
        # Replace existing dataroom for this channel
        if channel_id in self.datarooms:
            logger.warning(f"Replacing dataroom in channel {channel_id}")
            self.datarooms[channel_id].clear()
        
        # Create new dataroom for this channel
        self.datarooms[channel_id] = Dataroom(
            channel_id=channel_id,
            vector_db=ChromaDB(collection=f"channel_{channel_id}"),
            metadata={
                "display_name": display_name,
                "created_at": datetime.now(),
                "source_url": drive_url,
                "last_accessed": datetime.now()
            }
        )
        
        # Store enhanced metadata for listing
        self.channel_metadata[channel_id] = {
            "name": display_name,
            "sector": self.auto_detect_sector(documents),  # AI-extracted
            "stage": self.auto_detect_stage(documents),    # Series A, Seed, etc.
            "created_at": datetime.now(),
            "created_by": user_id,  # From Slack context
            "source_url": drive_url,
            "doc_count": len(documents),
            "doc_names": [doc.name for doc in documents[:3]]  # First 3 doc names
        }
        
        # Process documents
        self.process_documents(channel_id, drive_url)
        
        return f"üìä Dataroom '{display_name}' loaded and ready
üîó Use /summary and /ask commands"
        
    def generate_dataroom_name(self, drive_url: str) -> str:
        """Generate dataroom name from URL"""
        import re
        from datetime import datetime
        
        # Extract folder name or use timestamp
        folder_match = re.search(r'/folders/([^/?]+)', drive_url)
        if folder_match:
            base_name = folder_match.group(1)[:12]  # First 12 chars
        else:
            base_name = f"dataroom_{datetime.now().strftime('%m%d')}"
            
        return base_name
        
    def list_all_datarooms(self) -> str:
        """List all channel datarooms with rich metadata"""
        if not self.channel_metadata:
            return "üìÅ No datarooms loaded yet. Use '/load [url] [name]' to create one"
            
        result = "üìÅ Datarooms in this workspace:
"
        for channel_id, metadata in self.channel_metadata.items():
            channel_name = f"<#{channel_id}>"  # Slack channel reference
            dataroom_name = metadata["name"]
            
            # Rich metadata display
            sector_emoji = self.get_sector_emoji(metadata.get("sector", "Unknown"))
            created = metadata["created_at"].strftime("%b %d")
            created_by = metadata.get("created_by", "unknown")
            doc_count = metadata.get("doc_count", 0)
            
            result += f"‚Ä¢ {dataroom_name} in {channel_name}
"
            result += f"  {sector_emoji} {metadata.get('sector', 'Unknown')} ‚Ä¢ "
            result += f"üìÖ {created} ‚Ä¢ üë§ @{created_by} ‚Ä¢ üìÑ {doc_count} docs

"
            
        result += "Switch to any channel and use /summary or /ask"
        return result
        
    def get_sector_emoji(self, sector: str) -> str:
        """Get emoji for sector"""
        sector_emojis = {
            "fintech": "üíº", "e-commerce": "üõí", "saas": "‚ö°",
            "healthcare": "üè•", "edtech": "üìö", "proptech": "üè†",
            "mobility": "üöó", "enterprise": "üè¢", "consumer": "üë•",
            "unknown": "üîç"
        }
        return sector_emojis.get(sector.lower(), "üîç")
        
    def auto_detect_sector(self, documents) -> str:
        """AI-powered sector detection from documents"""
        # TODO: Implement AI extraction from document content
        # For MVP: return "Unknown" or let user specify
        return "Unknown"
        
    def auto_detect_stage(self, documents) -> str:
        """AI-powered funding stage detection"""
        # TODO: Implement AI extraction from document content
        # For MVP: return "Unknown" or let user specify  
        return "Unknown"
        
    def get_current_dataroom_info(self, channel_id: str) -> str:
        """Get info about current channel's dataroom"""
        if channel_id not in self.datarooms:
            return "‚ùå No dataroom loaded in this channel. Use '/load [url] [name]' first"
            
        metadata = self.channel_metadata[channel_id]
        dataroom = self.datarooms[channel_id]
        
        return f"üìä Current dataroom: '{metadata['name']}'
" \
               f"‚Ä¢ Loaded: {metadata['created_at'].strftime('%b %d at %H:%M')}
" \
               f"‚Ä¢ Documents: {len(dataroom.documents)} files
" \
               f"‚Ä¢ Ready for analysis"
        
    def connect_channel(self, channel_id: str, dataroom_name: str):
        """Connect a dataroom to a channel"""
        # Validation: Channel not already connected
        if channel_id in self.channel_connections:
            current_dataroom = self.channel_connections[channel_id]
            raise ValueError(f"Channel already connected to '{current_dataroom}'. Use /disconnect first")
            
        # Validation: Dataroom exists
        if dataroom_name not in self.datarooms:
            available = ', '.join(self.datarooms.keys())
            raise ValueError(f"Dataroom '{dataroom_name}' not found. Available: {available}")
        
        # Connect channel to dataroom
        self.channel_connections[channel_id] = dataroom_name
        self.datarooms[dataroom_name].last_accessed = datetime.now()
        
        # Check if dataroom is connected elsewhere
        other_channels = [
            ch for ch, dr in self.channel_connections.items() 
            if dr == dataroom_name and ch != channel_id
        ]
        
        response = f"üîó Connected to '{dataroom_name}' in this channel
üìä Ready for /summary and /ask commands"
        if other_channels:
            response += f"
‚ö†Ô∏è Note: This dataroom is also connected in {len(other_channels)} other channel(s)"
            
        return response
        
    def disconnect_channel(self, channel_id: str):
        """Disconnect channel from its current dataroom"""
        if channel_id not in self.channel_connections:
            return "‚ùå No dataroom connected to this channel"
            
        dataroom_name = self.channel_connections[channel_id]
        del self.channel_connections[channel_id]
        
        return f"‚úÖ Disconnected from '{dataroom_name}'
‚ÑπÔ∏è The dataroom remains available for future use"
        
    def get_channel_dataroom(self, channel_id: str) -> Optional[str]:
        """Get the dataroom connected to a channel"""
        return self.channel_connections.get(channel_id)
        
    def list_datarooms(self, channel_id: str) -> str:
        """List all available datarooms with connection status"""
        if not self.datarooms:
            return "üìÅ No datarooms available. Use '/import [url] [name]' to create one"
            
        current_dataroom = self.channel_connections.get(channel_id)
        result = "üìÅ Available Datarooms:
"
        
        for i, (name, dataroom) in enumerate(self.datarooms.items(), 1):
            # Connection status for this channel
            status = " ‚úÖ (connected here)" if name == current_dataroom else ""
            
            # Other connections
            other_connections = [
                ch for ch, dr in self.channel_connections.items() 
                if dr == name and ch != channel_id
            ]
            
            result += f"{i}. {name}{status}
"
            result += f"   ‚Ä¢ Created: {dataroom.created_at.strftime('%b %d')} by @{dataroom.created_by}
"
            result += f"   ‚Ä¢ Last accessed: {dataroom.last_accessed_relative()}
"
            
            if other_connections:
                result += f"   ‚Ä¢ Also connected in: {len(other_connections)} other channel(s)
"
            elif name != current_dataroom:
                result += f"   ‚Ä¢ Not connected anywhere
"
                
            result += "
"
            
        result += "Use '/use [name]' to work with a dataroom"
        return result
```

### 3.3 Typical Workflow Examples

#### Workflow 1: Initial Setup and Analysis

```bash
# PHASE 1: Simple load and analyze
[In #startup-x-dd]
User: /load https://drive.google.com/startupX "Startup X Series A"
Bot: üìä Dataroom 'Startup X Series A' loaded and ready (3 documents, 192 chunks)
     üîó Use /summary and /ask commands

# PHASE 2: Analysis (immediately available)

User: /summary
Bot: üìä DATAROOM ANALYSIS - Startup X Series A
     [Complete VC analysis with source attribution]

User: /ask What is the burn rate?
Bot: The burn rate is ‚Ç¨150k/month [Source: pitch_deck.pdf, page 18]
     Current runway: 18 months based on ‚Ç¨2.7M in the bank
```

#### Workflow 2: Context Switching

```bash
# PHASE 3: Different deal, different channel
[In #startup-y-dd]
User: /load https://drive.google.com/startupY "Startup Y Seed"
Bot: üìä Dataroom 'Startup Y Seed' loaded and ready (2 documents, 156 chunks)
     üîó Use /summary and /ask commands

User: /ask What is the TAM?
Bot: The TAM for Startup Y is ‚Ç¨2.5B [Source: pitch_deck.pdf, page 8]
     ‚Ä¢ SOM: ‚Ç¨150M over 5 years
     ‚Ä¢ Addressing e-commerce logistics in DACH region
```

#### Workflow 3: Parallel Analysis

```bash
# PHASE 4: Cross-channel discussion
[In #investment-committee]
User: /list-datarooms
Bot: üìÅ Datarooms in this workspace:
     ‚Ä¢ Startup X Series A in #startup-x-dd
     ‚Ä¢ Startup Y Seed in #startup-y-dd
     
     Switch to any channel and use /summary or /ask

[Team switches to #startup-x-dd to discuss the deal]

User: /ask Show me the competitive analysis
Bot: Based on the pitch deck, main competitors are:
     1. Competitor A (‚Ç¨50M raised) [Source: pitch_deck.pdf, page 14]
     2. Competitor B (Series B) [Source: pitch_deck.pdf, page 14]
     
[Meanwhile in #dd-startup-x - same dataroom, different discussion]
User: /ask What are the key risks?
Bot: Key risks identified in the dataroom:
     ‚Ä¢ Customer concentration: Top 3 customers = 60% revenue
     ‚Ä¢ Regulatory risk in Germany [Source: pitch_deck.pdf, page 22]
```

#### Workflow 4: System Management

```bash
# Check what's available
User: /list-datarooms
Bot: üìÅ Available Datarooms:
     1. Startup X Series A (Fintech) ‚úÖ (connected here)
        ‚Ä¢ Created: Jan 15 by @john
        ‚Ä¢ Last accessed: 2 hours ago
        ‚Ä¢ Also connected in: #investment-committee
        
     2. Startup Y Seed (E-commerce)
        ‚Ä¢ Created: Jan 10 by @sarah
        ‚Ä¢ Last accessed: 3 days ago
        ‚Ä¢ Not connected anywhere

# Check current status
User: /current
Bot: üìä Currently connected to: 'Startup X Series A'
     ‚Ä¢ Company: Startup X (Fintech)
     ‚Ä¢ Documents: 3 files (45.2 MB)
     ‚Ä¢ Last updated: 2 hours ago
     ‚Ä¢ Connected: 15 minutes ago
```

### 3.4 Single-Tenant Architecture Benefits

**1. Complete Data Isolation**: Each VC firm has their own dedicated instance
- Zero risk of data leakage between clients
- Sequoia's data never touches Andreessen's infrastructure
- Perfect for highly sensitive due diligence information

**2. Simplified Architecture**: No multi-tenancy complexity
- Channel ID = Dataroom (simple 1:1 mapping)
- No client segregation logic needed
- Fewer moving parts = more reliable system

**3. Customization per Client**: Each instance can be tailored
- Custom branding, integrations, workflows
- Client-specific LLM prompts and analysis templates
- Different security policies per VC firm

**4. Independent Scaling**: Each client scales separately
- Heavy users don't affect light users
- Dedicated resources per client
- Easier performance optimization

**5. Regulatory Compliance**: Easier to achieve
- Clear data boundaries for audits
- Client-specific compliance requirements
- Simpler backup and disaster recovery

**6. Natural Channel Workflow**: Intuitive for teams
- `/load` in #startup-x-dd creates dataroom for that deal
- Teams naturally organize by deals/channels
- No confusion about which dataroom is active

### 3.5 Simplified Business Rules

1. **One Channel = One Dataroom**: Simple mapping, no connection complexity
2. **Independent Instances**: Each VC firm has complete isolation
3. **Channel-Based Access**: Context is always clear from the channel
4. **Persistent Storage**: Datarooms survive until instance is reset

### 3.6 Stopper Solutions (MVP Strategy)

#### **Problem 1: Dataroom Updates - CRITICAL STOPPER**

**Scenario**: Startup sends updated pitch deck, user wants to refresh analysis

**MVP Solution: Simple Replacement Strategy**
```bash
# Original load
[In #startup-x-dd]
User: /load https://drive.google.com/v1 "Startup X Series A"
Bot: üìä Dataroom 'Startup X Series A' loaded and ready

# Week later: Updated documents
# SOLUTION: User manually updates Drive folder, then reloads
User: /load https://drive.google.com/v1-updated "Startup X Series A" 
Bot: ‚ö†Ô∏è Replacing existing dataroom 'Startup X Series A'
     üìä Dataroom 'Startup X Series A' loaded and ready (updated content)
     ‚ÑπÔ∏è Previous analysis history has been reset
```

**MVP Rules:**
1. **Complete replacement only** - no incremental updates
2. **No version history** - latest version overwrites all
3. **User responsibility** - manually organize Drive folder with ALL desired docs
4. **Analysis reset** - previous Q&A history is lost

**Why this works for MVP:**
- ‚úÖ Simple to implement (no version management complexity)
- ‚úÖ User controls exactly what gets included
- ‚úÖ No risk of stale data mixing with new data
- ‚úÖ Clear mental model: "reload = fresh start"

#### **Problem 2: Poor Dataroom Identification**

**Scenario**: New analyst sees generic names, can't identify deals

**Current Problem:**
```bash
User: /list-datarooms
Bot: üìÅ Datarooms in this workspace:
     ‚Ä¢ Series A in #deal-1          # ??? Which company?
     ‚Ä¢ Seed Round in #deal-2        # ??? What sector?
     ‚Ä¢ DD Project in #deal-3        # ??? Who's leading?
```

**MVP Solution: Rich Metadata Display**
```bash
User: /list-datarooms
Bot: üìÅ Datarooms in this workspace:
     ‚Ä¢ Startup X Series A in #startup-x-dd
       üíº Fintech ‚Ä¢ üìÖ Jan 15 ‚Ä¢ üë§ @john ‚Ä¢ üìÑ 3 docs
       
     ‚Ä¢ Startup Y Seed in #startup-y-dd  
       üõí E-commerce ‚Ä¢ üìÖ Jan 10 ‚Ä¢ üë§ @sarah ‚Ä¢ üìÑ 2 docs
       
     ‚Ä¢ Portfolio Co Z in #portfolio-review
       ‚ö° SaaS ‚Ä¢ üìÖ Jan 12 ‚Ä¢ üë§ @mike ‚Ä¢ üìÑ 5 docs

Use '/current' in any channel to see details
```

**Enhanced Metadata Strategy:**
```python
# When loading dataroom, extract metadata
metadata = {
    "display_name": "Startup X Series A",  # User-provided or auto-generated
    "sector": auto_detect_sector(documents),  # AI-extracted from docs
    "stage": auto_detect_stage(documents),   # Series A, Seed, etc.
    "created_by": user_id,
    "created_at": datetime.now(),
    "doc_count": len(documents),
    "channel_name": channel_name
}
```

#### **Problem 3: Channel Context Loss**

**Scenario**: User in wrong channel, doesn't know which dataroom is active

**Solution: Enhanced `/current` Command**
```bash
# In channel with no dataroom
User: /current  
Bot: ‚ùå No dataroom loaded in this channel
     üí° Use '/load [url] [name]' to create one
     üìã Use '/list-datarooms' to see all available datarooms

# In channel with dataroom
User: /current
Bot: üìä Current dataroom: 'Startup X Series A'
     üíº Fintech company, Series A stage
     üìÖ Loaded: Jan 15 at 14:30 by @john
     üìÑ Documents: pitch_deck.pdf, financials.xlsx, cap_table.pdf
     üîó Ready for /summary and /ask commands
```

### 3.6 Update Strategy (MVP)---

## 4. Technical Implementation Details

### 4.1 Document Processing Pipeline

```python
class DocumentProcessor:
    """Core document processing pipeline"""
    
    def process_document(self, file_path: str) -> ProcessedDocument:
        # Step 1: Parse with LlamaParse
        markdown = self.llama_parse_client.parse(
            file_path,
            result_type="markdown",
            target_pages=None,  # Process all pages
            invalidate_cache=True
        )
        
        # Step 2: Smart chunking
        chunks = self.markdown_chunker.split(
            markdown,
            chunk_size=512,
            chunk_overlap=50,
            respect_structure=True  # Don't split tables/lists
        )
        
        # Step 3: Generate embeddings
        embeddings = self.embedding_model.encode(chunks)
        
        # Step 4: Create metadata
        metadata = self.extract_metadata(chunks, file_path)
        
        return ProcessedDocument(
            chunks=chunks,
            embeddings=embeddings,
            metadata=metadata
        )
```

### 4.2 RAG Retrieval Strategy

```python
class RAGEngine:
    """Retrieval and generation engine"""
    
    def retrieve(self, query: str, top_k: int = 10) -> List[Chunk]:
        # Step 1: Embed query
        query_embedding = self.embed(query)
        
        # Step 2: Semantic search
        candidates = self.vector_db.search(
            query_embedding, 
            limit=top_k * 3  # Over-retrieve for re-ranking
        )
        
        # Step 3: Re-rank with cross-encoder
        reranked = self.reranker.rerank(
            query=query,
            documents=candidates,
            top_k=top_k
        )
        
        # Step 4: Fetch parent context
        with_context = self.expand_context(reranked)
        
        return with_context
    
    def generate(self, query: str, context: List[Chunk]) -> str:
        # Build prompt with context
        prompt = self.build_prompt(query, context)
        
        # Generate with GPT-4o
        response = self.llm.generate(
            prompt,
            temperature=0.1,  # Low for factual accuracy
            max_tokens=2000
        )
        
        return response
```

### 4.3 Chunking Strategies

```python
class MarkdownChunker:
    """Structure-aware markdown chunking"""
    
    def split(self, markdown: str) -> List[Chunk]:
        # Parse markdown structure
        ast = self.parse_markdown(markdown)
        
        chunks = []
        for element in ast:
            if element.type == "table":
                # Never split tables
                chunks.append(self.create_chunk(element, type="table"))
            elif element.type == "list":
                # Keep lists together if possible
                chunks.append(self.create_chunk(element, type="list"))
            elif element.type == "section":
                # Split sections if too large
                if len(element.text) > self.max_chunk_size:
                    chunks.extend(self.split_section(element))
                else:
                    chunks.append(self.create_chunk(element, type="section"))
        
        return chunks
```

---

## 5. Implementation Roadmap

### Phase 1: Foundation (Days 1-3)
**Goal**: Basic RAG pipeline working end-to-end

- [ ] Set up LlamaParse integration
- [ ] Implement basic chunking
- [ ] ChromaDB setup with persistence
- [ ] Basic /load command
- [ ] Simple retrieval pipeline

**Success Metrics**:
- Parse PDF successfully
- Store in vector DB
- Retrieve relevant chunks

### Phase 2: Core Commands (Days 4-6)
**Goal**: Implement all primary commands with quality output

- [ ] Complete /load with full pipeline
- [ ] Implement /summary with VC template
- [ ] Implement /ask with source attribution
- [ ] Add /status and /clear commands
- [ ] Multi-tenant channel isolation

**Success Metrics**:
- /summary extracts >90% of key metrics
- /ask provides accurate answers with sources
- Multiple channels work independently

### Phase 3: Quality & Optimization (Days 7-8)
**Goal**: Achieve commercial-grade quality

- [ ] Implement re-ranking
- [ ] Add parent document retrieval
- [ ] Optimize prompts for GPT-4o
- [ ] Add comprehensive error handling
- [ ] Performance optimization

**Success Metrics**:
- Response time <15 seconds
- Accuracy >95% on test queries
- Proper error messages

### Phase 4: Testing & Polish (Days 9-10)
**Goal**: Production readiness

- [ ] Test with problematic real datarooms
- [ ] Add logging and monitoring
- [ ] Documentation
- [ ] Deployment configuration
- [ ] User feedback integration

**Success Metrics**:
- Works with all test datarooms
- Clear user feedback
- Production deployed

---

## 6. Quality Metrics & Testing

### 6.1 Key Performance Indicators

| Metric | Target | Measurement |
|--------|--------|-------------|
| Data Extraction Accuracy | >90% | % of financial metrics correctly extracted |
| Source Attribution | 100% | Every fact has source document + page |
| Query Response Time | <15s | Time from /ask to response |
| Chunk Relevance | >0.8 | Cosine similarity of retrieved chunks |
| User Satisfaction | >4/5 | User feedback score |

### 6.2 Test Datarooms

Create test suite with:
1. Simple pitch deck (baseline)
2. Complex financial model (tables)
3. Multi-document dataroom (integration)
4. Visual-heavy deck (charts/graphs)
5. Poor quality scan (stress test)

### 6.3 Evaluation Framework

```python
class QualityEvaluator:
    """Automated quality testing"""
    
    test_queries = [
        "What is the current revenue?",
        "What is the burn rate and runway?",
        "Who are the founders and their backgrounds?",
        "What is the TAM/SAM/SOM?",
        "What are the key metrics and KPIs?"
    ]
    
    def evaluate_dataroom(self, dataroom_id: str) -> QualityReport:
        results = []
        for query in self.test_queries:
            response = self.rag_engine.ask(query)
            accuracy = self.check_accuracy(response, ground_truth)
            results.append(accuracy)
        
        return QualityReport(
            average_accuracy=np.mean(results),
            failed_queries=[q for q, r in zip(queries, results) if r < 0.8]
        )
```

---

## 7. File Structure

```
single-tenant-rag/
‚îú‚îÄ‚îÄ PROJECT_BRIEF.md           # This document
‚îú‚îÄ‚îÄ requirements.txt           # Dependencies
‚îú‚îÄ‚îÄ .env.example              # Environment template
‚îú‚îÄ‚îÄ app.py                    # Main Flask/Slack application (single-tenant)
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ parser.py             # LlamaParse integration
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py            # Markdown chunking
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py         # Embedding generation
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py       # ChromaDB wrapper (channel-based collections)
‚îÇ   ‚îú‚îÄ‚îÄ rag_engine.py         # RAG orchestration (simplified)
‚îÇ   ‚îî‚îÄ‚îÄ llm.py                # GPT-4o interface
‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ load_handler.py       # /load command (channel-based)
‚îÇ   ‚îú‚îÄ‚îÄ summary_handler.py    # /summary command
‚îÇ   ‚îú‚îÄ‚îÄ ask_handler.py        # /ask command
‚îÇ   ‚îî‚îÄ‚îÄ admin_handlers.py     # /current, /list-datarooms
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ channel_dataroom.py   # Simplified channel-dataroom model
‚îÇ   ‚îî‚îÄ‚îÄ document.py           # Document model
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ logger.py             # Logging configuration
‚îÇ   ‚îú‚îÄ‚îÄ config.py             # Single-tenant configuration
‚îÇ   ‚îî‚îÄ‚îÄ slack_formatter.py    # Slack message formatting
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_channel_pipeline.py
    ‚îú‚îÄ‚îÄ test_single_tenant.py
    ‚îî‚îÄ‚îÄ fixtures/
        ‚îî‚îÄ‚îÄ sample_deck.pdf
```

---

## 8. Configuration & Environment

### 8.1 Required Environment Variables

```bash
# API Keys
OPENAI_API_KEY=sk-...
LLAMA_CLOUD_API_KEY=llx-...
TAVILY_API_KEY=tvly-...  # For future enrichment

# Slack
SLACK_BOT_TOKEN=xoxb-...
SLACK_APP_TOKEN=xapp-...
SLACK_SIGNING_SECRET=...

# Google Drive
GOOGLE_SERVICE_ACCOUNT_JSON='{"type": "service_account"...}'

# Vector Database (Production)
PINECONE_API_KEY=...  # Optional for production
PINECONE_ENVIRONMENT=...

# Configuration
ENVIRONMENT=development|production
LOG_LEVEL=INFO
MAX_CHUNK_SIZE=512
EMBEDDING_MODEL=text-embedding-3-small
LLM_MODEL=gpt-4o
```

### 8.2 Dependencies

```python
# requirements.txt
flask==2.3.0
slack-bolt==1.18.0
llama-parse==0.3.0
llama-index==0.9.0
chromadb==0.4.0
openai==1.0.0
pydantic==2.0.0
numpy==1.24.0
python-dotenv==1.0.0
tenacity==8.2.0  # For retry logic
```

---

## 9. Risk Mitigation

### 9.1 Technical Risks

| Risk | Mitigation |
|------|------------|
| LlamaParse API downtime | Implement retry logic with exponential backoff |
| Large document processing time | Async processing with progress updates |
| Vector DB scaling issues | Use Pinecone for production |
| GPT-4o rate limits | Implement request queuing |
| Poor quality PDFs | Graceful degradation with user feedback |

### 9.2 Cost Management

Estimated costs per dataroom:
- LlamaParse: ~$0.003/page √ó 50 pages = $0.15
- Embeddings: ~$0.0001/1K tokens √ó 100K = $0.01
- GPT-4o: ~$0.01/1K tokens √ó 10K = $0.10
- **Total: ~$0.26 per dataroom analysis**

### 9.3 Fallback Strategies

```python
class FallbackHandler:
    """Graceful degradation for failures"""
    
    def process_with_fallback(self, document):
        try:
            # Primary: LlamaParse
            return self.llama_parse(document)
        except LlamaParseError:
            # Fallback: Direct GPT-4o (lower quality)
            logger.warning("LlamaParse failed, using GPT-4o direct")
            return self.gpt4_direct(document)
        except Exception as e:
            # Final: Error message
            return ErrorResponse(
                "Unable to process document. Please try again later.",
                error_id=str(e)
            )
```

---

## 10. Success Criteria

### 10.1 Minimum Viable Product (MVP)

The MVP is successful when:
- [ ] Can load a complete dataroom from Google Drive
- [ ] Extracts >90% of financial metrics accurately  
- [ ] Provides coherent answers to standard VC questions
- [ ] Maintains separate datarooms per Slack channel
- [ ] Response time <15 seconds
- [ ] Source attribution for all facts

### 10.2 Commercial Viability

The product is commercially viable when:
- [ ] VC analysts prefer it over manual analysis
- [ ] Reduces dataroom analysis time by >50%
- [ ] Provides consistent, high-quality outputs
- [ ] Handles edge cases gracefully
- [ ] Scales to multiple simultaneous users

---

## 11. Troubleshooting Common Issues

### **Issue: "No dataroom loaded in this channel"**
```bash
User: /summary
Bot: ‚ùå No dataroom loaded in this channel

SOLUTION:
User: /load https://drive.google.com/folder "Company Name"
# OR
User: /list-datarooms  # Check other channels
```

### **Issue: "Dataroom already exists with that name"**
```bash
User: /load https://drive.google.com/v2 "Startup X"  
Bot: ‚ö†Ô∏è Replacing existing dataroom 'Startup X'

This is INTENTIONAL for MVP:
‚Ä¢ Same name = automatic replacement
‚Ä¢ No versioning = simpler implementation
‚Ä¢ User controls updates via Drive folder management
```

### **Issue: "Can't find my dataroom"**
```bash
User: /list-datarooms
Bot: üìÅ Datarooms in this workspace:
     ‚Ä¢ CompanyA in #deal-1
     ‚Ä¢ CompanyB in #deal-2

SOLUTION: Datarooms are channel-specific
‚Ä¢ Check if you're in the right channel
‚Ä¢ Use /current to see what's loaded in current channel
```

### **Issue: "Poor extraction quality"**
```bash
User: /ask What's the revenue?
Bot: I couldn't find revenue information in the documents.

SOLUTIONS:
1. Check document quality (PDFs better than scans)
2. Ensure financial data is in text, not just images
3. Try rephrasing: "/ask Show me financial metrics"
4. Use /summary first to see what was extracted
```

### **Issue: "Slow processing"**
```bash
User: /load [large-drive-folder]
Bot: [Takes 5+ minutes]

EXPECTED BEHAVIOR:
‚Ä¢ 10-50 pages: 30-60 seconds  
‚Ä¢ 100+ pages: 2-5 minutes
‚Ä¢ Very large folders: Consider splitting into smaller batches
```

## 12. Next Steps

1. **Review and approve this brief**
2. **Set up development environment**  
3. **Obtain API keys (LlamaParse, OpenAI)**
4. **Begin Phase 1 implementation**
5. **Daily progress reviews**

---

## 12. Future Enhancements (Post-MVP)

### Version 2.0 Features

#### Advanced Update Management
- **Incremental updates**: Add/remove individual documents
- **Version history**: Track changes between versions
- **Diff comparison**: Compare dataroom versions
- **Automatic deduplication**: Detect unchanged documents

#### Search and Discovery
- **Global search**: `/search-all` across all datarooms
- **Smart tags**: Auto-categorization by sector, stage, metrics
- **Similar companies**: Find comparable startups in datarooms
- **Trend analysis**: Track metrics across portfolio

#### Collaboration Features
- **Access controls**: Restrict dataroom access by user/role
- **Audit trail**: Track who accessed what and when
- **Comments**: Add notes to specific dataroom sections
- **Shared queries**: Save and share common analyses

#### Data Management
- **Archive system**: Move old datarooms to cold storage
- **Usage analytics**: Track most/least used datarooms
- **Automatic cleanup**: Remove unused datarooms after X days
- **Export functionality**: Download analysis as PDF/Excel

#### Advanced Analysis
- **Comparison mode**: Side-by-side analysis of multiple datarooms
- **Portfolio view**: Aggregate metrics across all datarooms
- **Automated alerts**: Notify on key metric thresholds
- **Market enrichment**: Auto-append market research to datarooms

#### Technical Improvements
- **Async processing**: Background import with progress updates
- **Resume on failure**: Continue interrupted imports
- **Caching layer**: Speed up repeated queries
- **Multi-modal analysis**: Process images, charts, graphs

### Version 3.0 Vision
- **AI-generated due diligence**: Automated red flag detection
- **Integration with CRM**: Sync with portfolio management tools
- **Predictive analytics**: Success probability scoring
- **Natural language reports**: Generate LP updates from datarooms---

## Appendix A: Comparison with Current System

| Aspect | Current System | New RAG System |
|--------|----------------|----------------|
| Architecture | Direct PDF‚ÜíGPT | PDF‚ÜíParse‚ÜíChunk‚ÜíRAG‚ÜíGPT |
| Extraction Quality | <70% | >90% |
| Source Attribution | None | Every fact referenced |
| Persistence | Session only | Permanent vector DB |
| Multi-tenant | No | Yes (per channel) |
| Query Quality | Poor context | Rich context from RAG |
| Processing Time | 10-30s | 10-15s |
| Scalability | Limited | Highly scalable |
| Cost per Analysis | ~$0.50 | ~$0.26 |

## Appendix B: Technical Decision Rationale

### Why LlamaParse?
- Best balance of quality, speed, and ease of use
- Native Markdown output perfect for chunking
- Handles complex tables and layouts
- Simple API integration
- Cost-effective ($0.003/page)

### Why ChromaDB/Pinecone?
- ChromaDB: Perfect for development, embedded, simple
- Pinecone: Production-ready, scalable, managed service
- Both support metadata filtering for multi-tenancy

### Why LlamaIndex?
- Purpose-built for RAG applications
- Excellent chunking strategies
- Built-in re-ranking support
- Active community and documentation

### Why Not Build Parsing In-House?
- Complex engineering problem
- Maintenance burden
- LlamaParse already optimized
- Focus on business logic, not infrastructure

---

## Document Version
- **Version**: 1.0
- **Date**: January 2024
- **Author**: DataRoom Intelligence Team
- **Status**: DRAFT - Awaiting Approval

---

*END OF DOCUMENT*