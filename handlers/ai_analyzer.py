"""
AI Analysis handler for DataRoom Intelligence Bot
Integrates with OpenAI GPT-4 to analyze data room documents
"""

import json
import re
from typing import Dict, List, Any, Optional, Tuple
from openai import OpenAI
from config.settings import config
from prompts.analysis_prompts import DATAROOM_ANALYSIS_PROMPT, SCORING_PROMPT, SLACK_READY_ANALYSIS_PROMPT
from prompts.qa_prompts import QA_PROMPT, MEMO_PROMPT, GAPS_PROMPT, SLACK_READY_GAPS_PROMPT
from utils.logger import get_logger

logger = get_logger(__name__)

class AIAnalyzer:
    """Handles AI-powered analysis of data room documents using OpenAI GPT-4"""

    # ===================== Presentation helpers (analyst-friendly) =====================
    _EMOJI_ES2UNICODE = {
        # comunes (ES)
        ":p√°gina_boca_arriba:": "üìÑ", ":bombilla:": "üí°", ":gr√°fico_de_barras:": "üìä",
        ":espadas_cruzadas:": "‚öîÔ∏è", ":autopista:": "üõ£Ô∏è", ":cohete:": "üöÄ",
        ":bolsa_de_dinero:": "üí∞", ":bocadillo_de_di√°logo:": "üí¨",
        ":lupa:": "üîé", ":flechas_en_sentido_antihorario:": "üîÑ",
        ":carpeta_de_archivos:": "üóÇÔ∏è", ":libreta_de_notas:": "üìí", ":martillo:": "üî®",
        # comunes (EN)
        ":page_facing_up:": "üìÑ", ":bulb:": "üí°", ":bar_chart:": "üìä",
        ":crossed_swords:": "‚öîÔ∏è", ":motorway:": "üõ£Ô∏è", ":rocket:": "üöÄ",
        ":moneybag:": "üí∞", ":speech_balloon:": "üí¨", ":mag:": "üîé", ":arrows_counterclockwise:": "üîÑ",
        # previos
        ":dardo:": "üéØ", ":gr√°fico_de_pastel:": "ü•ß"
    }

    # Headers usados para inyecciones
    _H_VALUE = "**VALUE PROPOSITION:**"
    _H_MARKET = "**MARKET ANALYSIS:**"
    _H_COMPETITORS = "**COMPETITORS:**"
    _H_ROADMAP = "**PRODUCT ROADMAP:**"
    _H_GTM = "**GO-TO-MARKET STRATEGY:**"
    _H_FIN = "**FINANCIAL HIGHLIGHTS:**"

    @staticmethod
    def _strip_completeness_meta(text: str) -> str:
        # elimina cualquier l√≠nea con [COMPLETENESS-GUARD] ... (prompt leak)
        return re.sub(r"\[COMPLETENESS-GUARD][^\n]*\n?", "", text, flags=re.IGNORECASE)

    @staticmethod
    def _format_missing_sections_human(missing: List[str], slides_range: Tuple[int,int]) -> str:
        if not missing:
            return ""
        pretty = ", ".join(missing)
        a, b = slides_range
        return (
            f"‚ö†Ô∏è **Missing Sections in Deck**: {pretty}.\n"
            f"_These were not found across slides {a}‚Äì{b}. "
            f"Absence is unusual; treat as a diligence gap and request clarification._\n"
        )

    @classmethod
    def _map_emojis(cls, text: str) -> str:
        for src, uni in cls._EMOJI_ES2UNICODE.items():
            text = text.replace(src, uni)
        return text

    @staticmethod
    def _inject_exec_summary_top(text: str, extracted: Dict[str, Any]) -> str:
        """Inserta EXECUTIVE SUMMARY (3‚Äì5 bullets) al inicio usando se√±ales del JSON."""
        bullets: List[str] = []
        fin = (extracted.get("financials") or {}) if isinstance(extracted, dict) else {}
        gmv = fin.get("gmv") or []
        vat = fin.get("vat") or []
        funding = fin.get("funding_rounds") or []

        if re.search(r"\*\*VALUE PROPOSITION\*\*:", text, re.IGNORECASE):
            bullets.append("‚Ä¢ Clear value proposition articulated; confirm evidence of merchant demand.")
        if gmv:
            bullets.append(f"‚Ä¢ Demonstrates traction via GMV datapoints ({len(gmv)} mentions); verify conversion to revenue and margins.")
        if vat:
            bullets.append("‚Ä¢ Operates in regulated tax/VAT domain; diligence on compliance and government alignment required.")
        bullets.append("‚Ä¢ Competitive landscape absent in deck; request top comps and differentiation matrix.")
        if funding:
            fr = funding[0]
            round_s = fr.get("round", "Seed")
            bullets.append(f"‚Ä¢ {round_s} round present; request runway, use of funds, and next raise triggers.")

        if not bullets:
            return text
        exec_block = "üéØ **EXECUTIVE SUMMARY**\n" + "\n".join(bullets[:5]) + "\n\n"
        return exec_block + text

    @staticmethod
    def _inject_analyst_notes(text: str) -> str:
        """A√±ade micro-notas de analista tras bloques principales (forzado)."""
        def inject(after_title: str, note: str) -> str:
            pattern = rf"({re.escape(after_title)})\n"
            repl = r"\1\n" + note.strip() + "\n"
            return re.sub(pattern, repl, text, count=1, flags=re.IGNORECASE)

        t = text
        t = inject("**FINANCIAL HIGHLIGHTS:**",
                   "‚Ä¢ _Analyst Note:_ GMV/VAT ‚â† revenue. Request revenue model, gross margin profile, and unit economics clarity.")
        t = inject("**MARKET ANALYSIS:**",
                   "‚Ä¢ _Analyst Note:_ Provide TAM/SAM/SOM with sources; current evidence is insufficient for sizing conviction.")
        t = inject("**GO-TO-MARKET STRATEGY:**",
                   "‚Ä¢ _Analyst Note:_ Clarify ICP, sales motion, and partner channels; without this, ramp assumptions are speculative.")
        t = inject("**COMPETITORS:**",
                   "‚Ä¢ _Analyst Note:_ Missing landscape; ask for top 3‚Äì5 comps and differentiation on product, pricing, and regulatory posture.")
        t = inject("**PRODUCT ROADMAP:**",
                   "‚Ä¢ _Analyst Note:_ Outline milestones, regulatory checkpoints, and defensibility (data, integrations, gov links).")
        return t

    @staticmethod
    def _augment_inference(text: str, extracted: Dict[str, Any]) -> str:
        """Si faltan COMPETITORS/GTM/ROADMAP, inyecta un bloque inferido (marcado)."""
        def add_after(header: str, lines: List[str]) -> str:
            pattern = rf"({re.escape(header)}\s*\n)"
            repl = r"\1" + "\n".join(lines).strip() + "\n"
            return re.sub(pattern, repl, text, count=1, flags=re.IGNORECASE)

        t = text
        if not (extracted.get("competition") or []):
            t = add_after(AIAnalyzer._H_COMPETITORS, [
                "‚Ä¢ _Inference:_ Likely comps include Global Blue, Planet (tax-free solutions).",
                "‚Ä¢ _Action:_ Provide a differentiation matrix (product, take rate, compliance posture)."
            ])
        if not (extracted.get("gtm") or []):
            t = add_after(AIAnalyzer._H_GTM, [
                "‚Ä¢ _Inference:_ Expect ICP: mid/large retailers; channels: direct sales + POS/ERP partners.",
                "‚Ä¢ _Action:_ Share pipeline math (win rates, quotas) and partner agreements status."
            ])
        # Si existe el bloque Roadmap, a√±adimos gu√≠a; si no existe, no lo creamos de la nada
        if re.search(r"\*\*PRODUCT ROADMAP\*\*:", t, re.IGNORECASE):
            t = add_after(AIAnalyzer._H_ROADMAP, [
                "‚Ä¢ _Inference:_ Milestones should cover certifications, gov integrations, and merchant onboarding automation.",
                "‚Ä¢ _Action:_ Provide quarterly roadmap with measurable deliverables."
            ])
        return t

    @staticmethod
    def _improve_next_steps(text: str, missing: List[str]) -> str:
        # Reemplaza un bloque de "Next Steps" gen√©rico por acciones concretas
        actionable = [
            "‚Ä¢ Request 12‚Äì24m P&L with revenue breakdown and gross margin assumptions.",
            "‚Ä¢ Provide competitive landscape slide with top 3‚Äì5 comps and pricing.",
            "‚Ä¢ Share GTM plan: ICP, channels, quota assumptions, pipeline math.",
            "‚Ä¢ Provide cap table and runway post-seed; outline next raise triggers.",
        ]
        if "team" in missing:
            actionable.append("‚Ä¢ Add management bios with relevant domain/regulatory expertise.")
        if "risks" in missing:
            actionable.append("‚Ä¢ Provide key execution/regulatory risks with mitigations.")

        # Normaliza el header (Next Steps / NEXT STEPS / **Next Steps:** ...)
        header_regex = r"(\*\*Next Steps\*\*:|\*\*NEXT STEPS\*\*:|\*\*Next Steps:\*\*|\*\*NEXT STEPS:\*\*)"
        if re.search(header_regex, text, re.IGNORECASE):
            text = re.sub(
                header_regex + r"([\s\S]*?)(?=\n\*\*|$)",
                "**Next Steps:**\n" + "\n".join(actionable) + "\n",
                text,
                flags=re.IGNORECASE
            )
        else:
            # si no exist√≠a, lo a√±adimos al final
            text = text.rstrip() + "\n\n**Next Steps:**\n" + "\n".join(actionable) + "\n"
        return text

    # --- Normalizaci√≥n financiera m√≠nima inline (evita confusiones Revenue/GMV/VAT/Funding)
    @staticmethod
    def _normalize_financials(fin: Dict[str, Any]) -> Dict[str, Any]:
        fin = fin or {}
        fin.setdefault("financials", {})
        f = fin["financials"]
        for k in ["revenue","gmv","vat","funding_rounds"]:
            f.setdefault(k, [])
        # Reglas b√°sicas de reclasificaci√≥n por notas/textos comunes
        def move_if(patterns: List[str], src: str, dst: str):
            keep = []
            moved = []
            for item in f.get(src, []):
                note = (item.get("note") or item.get("value") or "").lower()
                if any(p in note for p in patterns):
                    moved.append(item)
                else:
                    keep.append(item)
            f[src] = keep
            f[dst] = f.get(dst, []) + moved
        move_if(["eligible sales","gmv","gross merchandise"], "revenue", "gmv")
        move_if(["vat","iva"], "revenue", "vat")
        return fin

    def __init__(self):
        self.client = OpenAI(api_key=config.OPENAI_API_KEY)
        # ‚úÖ Alinear con extractor: usar siempre gpt-4o
        self.model = "gpt-4o"
        self.current_analysis = None
        self.analysis_context = None

    def analyze_dataroom(self, processed_documents: List[Dict[str, Any]],
                        document_summary: Dict[str, Any]) -> Dict[str, Any]:
        """Perform comprehensive data room analysis using GPT-4"""
        try:
            logger.info("üß† Starting AI analysis of data room...")

            # Prepare context for analysis
            context = self._prepare_analysis_context(processed_documents, document_summary)

            # PHASE 1: Extract financial data deterministically
            logger.info("üí∞ Extracting financial data patterns...")
            from utils.financial_extractor import extract_financial_data, format_financial_data_for_prompt
            
            financial_data = extract_financial_data(context['full_content'])
            formatted_financials = format_financial_data_for_prompt(financial_data)
            
            # Use new Slack-ready prompt (TESTING - Step 2)
            # Completeness guard: si faltan secciones, ind√≠calo en el prompt
            extracted = {}
            try:
                extracted = json.loads(context['full_content'])
            except:
                pass

            must_sections = ["team","competition","risks","why_now"]
            missing = [s for s in must_sections if not extracted.get(s)]

            analysis_prompt = SLACK_READY_ANALYSIS_PROMPT.format(
                documents_with_metadata=context['documents_summary'],
                document_contents=context['full_content'],  # ‚ùå sin truncamiento
                extracted_financials=formatted_financials
            )

            if missing:
                analysis_prompt += (
                    "\n\n[COMPLETENESS-GUARD] The deck appears to be missing sections: "
                    + ", ".join(missing)
                    + ". If truly not in deck, write 'Not found in deck' and reference slides scanned."
                )

            logger.info("üß™ TESTING: Using new Slack-ready prompt")

            # Call GPT-4o for analysis
            response = self.client.chat.completions.create(
                model=self.model,  # ser√° gpt-4o (ver __init__)
                messages=[
                    {"role": "system", "content": "You are a senior venture capital analyst with 15+ years of experience in due diligence and startup evaluation."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=4000,
                temperature=0.25
            )



            # Get the raw analysis
            analysis_result = response.choices[0].message.content

            # ---------- PRESENTATION LAYER ----------
            # 1) Ocultar meta y a√±adir nota humana de secciones faltantes
            analysis_result = self._strip_completeness_meta(analysis_result)

            # 2) Emojis 100% Unicode
            analysis_result = self._map_emojis(analysis_result)
            # Limpieza adicional de alias residuales :...:
            analysis_result = re.sub(r":([a-z0-9_+-]+):", "", analysis_result)

            # 3) Sustituir mensajes gen√©ricos por "Not found in deck (‚Ä¶)"
            analysis_result = re.sub(r"\b(No specific .*? mentioned\.)", "Not found in deck (explicitly verified).", analysis_result, flags=re.IGNORECASE)

            # 4) Nota "Missing Sections" arriba del todo si aplica
            slides_range = (1, max(extracted.get("slides_covered", [1])) if isinstance(extracted.get("slides_covered"), list) else 20)
            human_missing = self._format_missing_sections_human(missing, slides_range if isinstance(slides_range, tuple) else (1,20))
            if human_missing:
                analysis_result = human_missing + "\n" + analysis_result

            # 5) Executive Summary arriba
            analysis_result = self._inject_exec_summary_top(analysis_result, extracted)

            # 6) Analyst Notes por secci√≥n
            analysis_result = self._inject_analyst_notes(analysis_result)

            # 7) Inferencia m√≠nima para huecos
            analysis_result = self._augment_inference(analysis_result, extracted)

            # 8) Next Steps accionables
            analysis_result = self._improve_next_steps(analysis_result, missing)

            # Debug: Log the raw AI response
            logger.info(f"üîç SLACK-READY RESPONSE (length: {len(analysis_result)} chars)")
            logger.info(f"‚ú® Direct output - no parsing needed!")

            # Store for future Q&A
            self.current_analysis = analysis_result
            self.analysis_context = context

            logger.info("‚úÖ AI analysis completed successfully")
            logger.info(f"üîç RETURNING STRING (type: {type(analysis_result)}, length: {len(analysis_result)})")
            return analysis_result

        except Exception as e:
            logger.error(f"‚ùå AI analysis failed: {e}")
            import traceback
            logger.error(f"üîç FULL TRACEBACK: {traceback.format_exc()}")
            return {
                'error': str(e),
                'executive_summary': ['Analysis failed due to technical error'],
                'scoring': {},
                'red_flags': ['Could not complete analysis'],
                'missing_info': ['Analysis incomplete'],
                'recommendation': 'TECHNICAL_ERROR'
            }

    def _prepare_analysis_context(self, processed_documents: List[Dict[str, Any]],
                                 document_summary: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare structured context for AI analysis"""

        # Document metadata summary
        docs_summary = []
        full_content = ""

        for doc in processed_documents:
            if doc['type'] != 'error' and doc.get('content'):
                docs_summary.append({
                    'name': doc['name'],
                    'type': doc['type'],
                    'content_length': len(doc['content']),
                    'metadata': doc.get('metadata', {})
                })

                # Add content with document separator
                full_content += f"\n\n=== DOCUMENT: {doc['name']} ===\n"
                full_content += doc['content']  # ‚úÖ sin truncamiento: preserva fidelidad para el an√°lisis

        # Si el contenido ya es JSON de extractor, normal√≠zalo antes
        extracted: Dict[str, Any] = {}
        try:
            # Si tenemos solo un documento y es JSON, procesarlo
            if len(processed_documents) == 1 and processed_documents[0].get('type') == 'pdf':
                extracted = json.loads(full_content.split('=== DOCUMENT:')[1].split('===')[1].strip())
        except Exception:
            pass

        if isinstance(extracted, dict) and "financials" in extracted:
            extracted = self._normalize_financials(extracted)
            full_content = f"\n\n=== DOCUMENT: {processed_documents[0]['name']} ===\n{json.dumps(extracted, ensure_ascii=False)}"

        return {
            'documents_summary': json.dumps(docs_summary, indent=2),
            'full_content': full_content,
            'document_count': len(docs_summary),
            'total_content_length': len(full_content)
        }


    def answer_question(self, question: str) -> str:
        """Answer specific questions about the analyzed data room"""
        try:
            if not self.current_analysis or not self.analysis_context:
                return "‚ùå No data room has been analyzed yet. Please run /analyze first."

            logger.info(f"ü§î Answering question: {question[:100]}...")

            # Extract financial data for Q&A context
            from utils.financial_extractor import extract_financial_data, format_financial_data_for_prompt
            financial_data = extract_financial_data(self.analysis_context['full_content'])
            formatted_financials = format_financial_data_for_prompt(financial_data)

            # Create Q&A prompt with FULL CONTENT and EXTRACTED FINANCIAL DATA
            qa_prompt = QA_PROMPT.format(
                analyzed_documents_summary=self.analysis_context['full_content'],  # ‚Üê CONTENIDO REAL sin truncamiento
                extracted_financials=formatted_financials,
                user_question=question
            )

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert VC analyst who has just completed a comprehensive data room analysis."},
                    {"role": "user", "content": qa_prompt}
                ],
                max_tokens=500,
                temperature=0.2
            )

            answer = response.choices[0].message.content
            logger.info("‚úÖ Question answered successfully")
            return answer

        except Exception as e:
            logger.error(f"‚ùå Failed to answer question: {e}")
            return f"‚ùå Sorry, I couldn't answer that question due to a technical error: {str(e)}"


    def analyze_gaps(self) -> str:
        """Analyze information gaps using Slack-Ready approach with Critical Gaps"""
        try:
            if not self.current_analysis or not self.analysis_context:
                return "‚ùå No data room has been analyzed yet. Please run `/analyze` first."

            logger.info("üîç Using new Slack-Ready gaps analysis with Critical Gaps...")

            # Extract financial data deterministically
            from utils.financial_extractor import extract_financial_data, format_financial_data_for_prompt
            financial_data = extract_financial_data(self.analysis_context['full_content'])
            formatted_financials = format_financial_data_for_prompt(financial_data)

            # Prepare analysis summary (simplified for Slack-Ready)
            analysis_summary = json.dumps({
                'content_type': 'slack_ready_analysis',
                'content_length': self.current_analysis.get('content_length', 0),
                'analysis_sections': 'All sections successfully extracted'
            }, indent=2)

            # Use new Slack-Ready gaps prompt with Critical Gaps
            gaps_prompt = SLACK_READY_GAPS_PROMPT.format(
                analysis_summary=analysis_summary,
                extracted_financials=formatted_financials,
                documents_summary=self.analysis_context['full_content']  # Pass ACTUAL CONTENT like /ask does sin truncamiento
            )

            logger.info("üö® TESTING: Using Slack-Ready gaps prompt with Critical Gaps distinction")

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a senior VC analyst providing professional gaps analysis ready for Slack display."},
                    {"role": "user", "content": gaps_prompt}
                ],
                max_tokens=800,  # Smaller for Slack format
                temperature=0.2
            )

            # Return the Slack-ready gaps analysis directly
            gaps_analysis = response.choices[0].message.content

            logger.info(f"üîç SLACK-READY GAPS (length: {len(gaps_analysis)} chars)")
            logger.info("‚ú® Direct gaps output with Critical Gaps - no parsing needed!")

            return gaps_analysis

        except Exception as e:
            logger.error(f"‚ùå Failed to analyze gaps: {e}")
            return f"‚ùå Sorry, I couldn't analyze gaps due to a technical error: {str(e)}"


    def reset_analysis(self):
        """Reset current analysis context"""
        self.current_analysis = None
        self.analysis_context = None
        logger.info("üîÑ Analysis context reset")
