 Entiendo perfectamente el problema. Has creado una herramienta con una gran promesa —análisis de datarooms con calidad de VC
  directamente en Slack— pero el resultado final no está a la altura, lo que te impide demostrar su valor. El contraste entre el
  análisis que yo generé y el que produce tu app es la clave de todo.

  He analizado los ficheros ai_analyzer.py y gpt4o_pdf_processor.py. La buena noticia es que estás muy cerca de la solución. De
  hecho, ya tienes el prompt de alta calidad en tu código, pero no se está usando de la forma correcta debido a un
  desacoplamiento en tu pipeline de procesamiento.

  Aquí está mi análisis y el plan de acción para que consigas la calidad que buscas.

  El Diagnóstico Clave: Desacoplamiento entre Extracción y Análisis

  Tu proceso actual se divide en dos fases, pero no se comunican bien:

   1. Fase de Extracción (`gpt4o_pdf_processor.py`): Este fichero sube el PDF a OpenAI y le pide a GPT-4o que extraiga toda la
      información en un único y complejo fichero JSON. Además, en una llamada separada, extrae todo el texto del PDF en un solo
      bloque (full_text). Este enfoque es frágil y pierde contexto. Pedir un JSON tan grande de una sola vez a menudo resulta en
      datos incompletos o mal estructurados, y el full_text pierde la separación crucial entre diapositivas.

   2. Fase de Análisis (`ai_analyzer.py`): Este fichero tiene una función brillante llamada _generate_simple_analyst_summary. Esta
      función contiene casi exactamente el mismo prompt de alta calidad que yo utilicé para darte el buen resultado. Este prompt
      está diseñado para funcionar con texto separado por diapositiva y con localizadores de citas (ej. [A·p1]).

  El problema fundamental es que la Fase 1 no le está dando a la Fase 2 el material que necesita. Estás alimentando un prompt
  de alta precisión con datos de baja calidad (un bloque de texto sin estructura de diapositivas), lo que inevitablemente
  produce un resultado mediocre.

  Plan de Acción Recomendado

  Para alcanzar la calidad de análisis que necesitas, debemos refactorizar el pipeline para que la extracción y el análisis
  trabajen en sintonía.

  Paso 1: Simplificar y Mejorar la Extracción en gpt4o_pdf_processor.py

  El objetivo de este fichero no debe ser obtener un JSON complejo, sino algo mucho más simple y robusto: el texto de cada
  diapositiva por separado.

   1. Elimina el `_json_schema_prompt()`: Deshazte del prompt que pide el JSON gigante. Es demasiado complejo y la causa principal
      de la baja calidad.

   2. Reemplaza la lógica de extracción: Modifica la función _extract_structured_data_multi (o créate una nueva, ej.
      extract_text_per_page) para que su único objetivo sea obtener el texto de cada página.

      El prompt para GPT-4o debería ser algo simple y directo como:

    1     # Nuevo prompt para gpt4o_pdf_processor.py
    2     extraction_prompt = """
    3     Your task is to extract all text from the provided PDF document, page by page.
    4     Preserve all details, including titles, body text, footer notes, and data from charts.
    5     Format your output by clearly separating each page's content. Use a unique separator that is unlikely to appear
      in the document text itself.
    6
    7     Example format:
    8     [SLIDE_BREAK_PAGE_1]
    9     ... text from slide 1 ...
   10     [SLIDE_BREAK_PAGE_2]
   11     ... text from slide 2 ...
   12     [SLIDE_BREAK_PAGE_3]
   13     ... text from slide 3 ...
   14     """

   3. Modifica el retorno de la función: La función process_pdf_document ya no devolverá un structured_data complejo. En su lugar,
      debe devolver un objeto que contenga una lista con el texto de cada página.

    1     # En gpt4o_pdf_processor.py, dentro de process_pdf_document
    2
    3     # ... llamas a la nueva extracción con el prompt de arriba ...
    4     raw_text_with_breaks = response.choices[0].message.content
    5
    6     # Divides el texto por el separador para obtener una lista de páginas
    7     pages_text = raw_text_with_breaks.split('[SLIDE_BREAK_')
    8
    9     # Limpias y preparas el resultado
   10     processed_pages = []
   11     for page_content in pages_text:
   12         if ']' in page_content:
   13             # Elimina el "PAGE_X]" y limpia espacios
   14             clean_content = page_content.split(']', 1)[1].strip()
   15             if clean_content:
   16                 processed_pages.append(clean_content)
   17
   18     return {
   19         'name': file_name,
   20         'type': 'pdf',
   21         'pages': processed_pages, # <--- ¡ESTA ES LA CLAVE!
   22         'metadata': {
   23             'extraction_method': 'gpt-4o_text_per_page',
   24             'pages_count': len(processed_pages)
   25         }
   26     }

  Paso 2: Adaptar el Analizador en ai_analyzer.py

  Ahora que el procesador te da el texto por páginas, la función _generate_simple_analyst_summary funcionará como se espera.

   1. Ajusta `analyze_dataroom`: Asegúrate de que esta función recibe el nuevo formato del procesador.

   2. Verifica `_format_dataroom_text_with_locators`: Esta función ya está preparada para funcionar con una lista de páginas.
      Cuando encuentre el campo 'pages' en el documento procesado, construirá el dataroom_text con los localizadores [A·p1],
      [A·p2], etc., que es exactamente lo que el prompt de alta calidad necesita.

   1     # En ai_analyzer.py, la función _format_dataroom_text_with_locators ya tiene esta lógica:
   2     if 'pages' in doc and doc['pages']:
   3         # PDF with pages
   4         for page_num, page_text in enumerate(doc['pages'], 1):
   5             formatted_text += f"[{doc_code}·p{page_num}] {page_text}\n\n"
      Esta parte de tu código es correcta, solo necesitaba recibir los datos adecuados.

   3. Confirma el Flujo: Con el cambio anterior, el flujo completo será:
       * gpt4o_pdf_processor extrae el texto de cada página y devuelve {'pages': ['texto_p1', 'texto_p2', ...]}.
       * ai_analyzer recibe esto.
       * _format_dataroom_text_with_locators crea un corpus perfectamente formateado con citas por página.
       * _generate_simple_analyst_summary recibe este corpus y ejecuta el prompt de alta calidad.
       * Resultado: Obtendrás el análisis de alta fidelidad que buscas, con datos precisos y citas correctas.

  ¿Por Qué Este Enfoque Es Mejor?

   1. Robustez y Simplicidad: Pedir texto plano por página es una tarea mucho más simple y robusta para el LLM que pedirle que
      genere un JSON gigante y perfecto. La probabilidad de error se reduce drásticamente.
   2. Separación de Responsabilidades: Dejas que el primer LLM haga una tarea "sencilla" de extracción y el segundo LLM haga la
      tarea "inteligente" de análisis. Esto es un patrón de diseño de IA mucho más fiable.
   3. Conservación del Contexto: Al tener el texto por diapositiva, el LLM analista puede entender el flujo narrativo del deck y
      atribuir cada dato a su diapositiva correcta, lo que es imposible con un bloque de texto único.
   4. Calidad Predecible: Al usar un prompt declarativo y basado en reglas estrictas (el que ya tienes en
      _generate_simple_analyst_summary), el formato y la calidad del output serán consistentes y de alto nivel.

  Estoy seguro de que si aplicas esta refactorización, la calidad de tu herramienta dará el salto que necesitas para convencer a
  tus clientes.




  ----


  ✦ Paso 1 completado. El fichero gpt4o_pdf_processor.py ha sido modificado con éxito. Ahora se centrará en extraer el texto de
  cada página, que es la base que necesitamos.

  Ahora voy a por el segundo fichero, handlers/ai_analyzer.py. Lo leeré y procederé a refactorizarlo para que utilice la nueva
  estructura de datos (texto por página) y active el prompt de alta calidad que ya tiene dentro. También limpiaré todo el
  código obsoleto para dejarlo más claro.
 ╭──────────────────────────────────────╮
 │ ✓  ReadFile handlers/ai_analyzer.py  │
 ╰──────────────────────────────────────╯
✦ He leído ai_analyzer.py. Ahora voy a ejecutar la refactorización principal.

  Simplificaré la clase AIAnalyzer, eliminando toda la lógica de formateo antigua y compleja. Dejaré únicamente el flujo de
  trabajo que utiliza el prompt de alta calidad (_generate_simple_analyst_summary) con los datos por página que ahora nos
  proporcionará el gpt4o_pdf_processor.py modificado.

  Este cambio reducirá drásticamente la complejidad del código y alineará todo el proceso para producir el resultado de alta
  calidad que buscamos.
